{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDA, Exploratory Data Analysis\n",
    "===\n",
    "latitude/logitude are given by unknown consersion. Suppose that this conversion formula still kept the original infomation, how could we extract their usefule information in prediction? As well-known knowledge, house price is dependent on the region where their locate; this is why we have to consider lat/lon infomation seriously.\n",
    "1. nan conversion\n",
    "2. target, $\\mathbf{y\\Rightarrow\\log(1+y)}$ (`np.log1p`), for normalize fitting; later, back by $\\mathbf{y_p \\Rightarrow \\exp(y_p)-1}$ (`np.expm1`); or use `np.log` directly if all the target data are greater than 0.\n",
    "- latitude/longitude conversion, a°). knn means, b°). dbscans, then one-hot converstion\n",
    "  ```Lasso, 0.7012 ➡︎ 0.6893```, the last one can not assign a fixed value to fix the data.\n",
    "- different models, xgb, lgb, ...; here we try the `lightgbm`;\n",
    "- stack model, blend moder, ...; install `mlxtend` by pip.\n",
    "\n",
    "Note\n",
    "---\n",
    "1. Since 2019/05/17, two new implementations of gradient boosting trees: `ensemble.HistGradientBoostingClassifier` and `ensemble.HistGradientBoostingRegressor` , are supported by `scikit_learning`-0.21.1.\n",
    "```python\n",
    "# usage\n",
    "from sklearn.experimental import enable_hist_gradient_boosting \n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "HistGradientBoostingRegressor(loss=’least_squares’, learning_rate=0.1, max_iter=100, max_leaf_nodes=31, max_depth=None, min_samples_leaf=20, l2_regularization=0.0, max_bins=256, scoring=None, validation_fraction=0.1, n_iter_no_change=None, tol=1e-07, verbose=0, random_state=None)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm,tqdm_notebook\n",
    "import folium\n",
    "\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../dataset/train.csv')\n",
    "test_df = pd.read_csv('../dataset/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Train: \",train_df.shape[0],\"sales, and \",train_df.shape[1],\"features\")\n",
    "print (\"Test:  \",test_df.shape[0],\"sales, and \",test_df.shape[1],\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillnan(df_1,df_2,features):\n",
    "    df_1[features[0]].fillna(value=0,inplace=True)\n",
    "    df_2[features[0]].fillna(value=0,inplace=True)\n",
    "    df_1[features[1]].fillna(value=0,inplace=True)\n",
    "    df_2[features[1]].fillna(value=0,inplace=True)\n",
    "    \n",
    "    df=pd.concat([df_1[features],df_2[features]],axis=0)\n",
    "    for f in features[2:]:\n",
    "        df_1[f].fillna(value=df[f].median(),inplace=True)\n",
    "        df_2[f].fillna(value=df[f].median(),inplace=True)\n",
    "    return df_1,df_2\n",
    "\n",
    "nan_features=['parking_area','parking_price','txn_floor','village_income_median']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df,test_df=fillnan(train_df,test_df,nan_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ToDo\n",
    "---\n",
    "Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data\n",
    "---\n",
    "1. Quantitative\n",
    "   - time-state: 'txn_dt', 'building_complete_dt'\n",
    "   - non-time,\n",
    "           'parking_price','building_area','village_income_median','town_population','town_area',\n",
    "           'town_population_density','I_Min','II_MIN','III_MIN','IV_MIN','V_MIN','VI_MIN',\n",
    "           'VII_MIN','VIII_MIN','IX_MIN','X_MIN','XI_MIN','XII_MIN','XIII_MIN','XIV_MIN',\n",
    "   - location: 'lon','lat'\n",
    "- Qualitative\n",
    "  - building_material(9),city(11),total_floor(29),building_type(5),building_use(10),parking_way,\n",
    "    parking_area, txn_floor,'doc_rate', 'master_rate', 'bachelor_rate', 'jobschool_rate',\n",
    "       'highschool_rate', 'junior_rate', 'elementary_rate', 'born_rate',\n",
    "       'death_rate', 'marriage_rate', 'divorce_rate'\n",
    "  - village(2899)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantitative = ['txn_dt', 'building_complete_dt','parking_price','building_area','village_income_median','town_population','town_area',\n",
    "           'town_population_density','I_MIN','II_MIN','III_MIN','IV_MIN','V_MIN','VI_MIN',\n",
    "           'VII_MIN','VIII_MIN','IX_MIN','X_MIN','XI_MIN','XII_MIN','XIII_MIN','XIV_MIN',\n",
    "           'lon','lat']\n",
    "qualitative=['building_material','city','total_floor','building_type','building_use',\n",
    "             'parking_way','parking_area','txn_floor','doc_rate', 'master_rate', \n",
    "             'bachelor_rate', 'jobschool_rate','highschool_rate', 'junior_rate', \n",
    "             'elementary_rate', 'born_rate','death_rate', 'marriage_rate', 'divorce_rate',\n",
    "             'village']\n",
    "target=['total_price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Normality test\n",
    "---\n",
    "For quanntitative features, do the features follow normal distributed? The Shapior test,  `scipy.stats.shapiro`, does help to filter out the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['total_price_log']=np.log(train_df['total_price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_check(y,kind='log'):\n",
    "    if kind=='log':\n",
    "       y_c=np.log(y)\n",
    "    else:\n",
    "       y_c=y \n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.subplot(121)\n",
    "    sns.distplot(y_c , fit=norm);\n",
    "    (mu, sigma) = norm.fit(y_c)\n",
    "    #print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
    "    plt.legend(['Normal dist. ($\\mu:$ {:.2f}, $\\sigma:$ {:.2f} )'.format(mu, sigma)],fontsize=14,\n",
    "            loc='best')\n",
    "    plt.title('Convert by %s' %kind)\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    #fig = plt.figure(figsize=[8,6])\n",
    "    res = stats.probplot(y_c, plot=plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=(train_df['total_price'])\n",
    "\n",
    "dist_check(y,kind='log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_check(train_df['town_population'],kind='l')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skewness and Kurtosis\n",
    "---\n",
    "\\begin{eqnarray}\n",
    "    \\text{Skewness }&=&\\frac{E(X-\\mu)^3}{\\sigma^3}\\\\\n",
    "    \\text{Kurtosis }&=&\\frac{E(X-\\mu)^4}{\\sigma^4}\n",
    "\\end{eqnarray}\n",
    "1. Skewness $>0$, log-tail on the right-side, and $<0$, on the left side,\n",
    "- Kurtosis large, steep in the peak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# too large for data skewness and kurtosis\n",
    "print(\"Skewness: %f\" % train_df['total_price'].skew()) \n",
    "print(\"Kurtosis: %f\" % train_df['total_price'].kurt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try another one in log degree, look ...\n",
    "print(\"Skewness: %f\" % train_df['total_price_log'].skew()) \n",
    "print(\"Kurtosis: %f\" % train_df['total_price_log'].kurt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try another one in log degree, look ...\n",
    "print(\"Skewness: %f\" % train_df['town_population'].skew()) \n",
    "print(\"Kurtosis: %f\" % train_df['town_population'].kurt())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression assumes that data should follow the `normal distributed` rule. Thus, we want to check the  quantitative features, continuous variables, whether obey the normal rule;\n",
    "```\n",
    "        convert the feature, with skew >1, by logarithm, ln(var);\n",
    "        translate the data if data is smaller than 0.\n",
    "```        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check degree of skew \n",
    "skew_var_X={}\n",
    "for i in range(len(quantitative)):\n",
    "    skew_var_X[i]=abs(train_df[quantitative[i]].skew())\n",
    "skew_df=pd.Series(skew_var_X)#.sort_values(ascending=False)\n",
    "skew_df.index=quantitative\n",
    "skew_df.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_skew=train_df.copy()\n",
    "var_X_ln=skew_df.index[skew_df>1]\n",
    "for i in var_X_ln:\n",
    "    if min(train_df_skew[i])<=0:\n",
    "       train_df_skew[i]=np.log(train_df_skew[i]+abs(train_df_skew[i])+0.01) \n",
    "    else:\n",
    "       train_df_skew[i]=np.log(train_df_skew[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Practice\n",
    "---\n",
    "1. As above, Process test_df data;\n",
    "- Repeate Lasso regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC,ridge\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from mlxtend.regressor import StackingCVRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\n",
    "\n",
    "#Validation function\n",
    "n_folds = 5\n",
    "\n",
    "def rmsle_cv(model,X,y):\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(X.values)\n",
    "    rmse= np.sqrt(-cross_val_score(model, X.values, y, scoring=\"neg_mean_squared_error\", cv = kf))\n",
    "    return(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0001, random_state=1,tol=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_skew.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the X-y\n",
    "X =\n",
    "y = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = (rmsle_cv(lasso, X,y))\n",
    "print(\"\\nLasso score (alpha = {:.5f}): μ {:.4f} (𝝈 {:.4f})\\n\".format(0.0001,score.mean(), score.std()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
