{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDA, Exploratory Data Analysis\n",
    "===\n",
    "latitude/logitude are given by unknown consersion. Suppose that this conversion formula still kept the original infomation, how could we extract their usefule information in prediction? As well-known knowledge, house price is dependent on the region where their locate; this is why we have to consider lat/lon infomation seriously.\n",
    "1. nan conversion\n",
    "2. target, $\\mathbf{y\\Rightarrow\\log(1+y)}$ (`np.log1p`), for normalize fitting; later, back by $\\mathbf{y_p \\Rightarrow \\exp(y_p)-1}$ (`np.expm1`)\n",
    "- latitude/longitude conversion, a°). knn means, b°). dbscans, then one-hot converstion\n",
    "  ```Lasso, 0.7012 ➡︎ 0.6893```, the last one can not assign a fixed value to fix the data.\n",
    "- different models, xgb, lgb, ...; here we try the `lightgbm`;\n",
    "- stack model, blend moder, ...; install `mlxtend` by pip.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm,tqdm_notebook\n",
    "import folium\n",
    "\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../dataset/train.csv')\n",
    "test_df = pd.read_csv('../dataset/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Train: \",train_df.shape[0],\"sales, and \",train_df.shape[1],\"features\")\n",
    "print (\"Test:  \",test_df.shape[0],\"sales, and \",test_df.shape[1],\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data\n",
    "---\n",
    "1. Quantitative\n",
    "   - time-state: 'txn_dt', 'building_complete_dt'\n",
    "   - non-time,\n",
    "           'parking_price','building_area','village_income_median','town_population','town_area',\n",
    "           'town_population_density','I_Min','II_MIN','III_MIN','IV_MIN','V_MIN','VI_MIN',\n",
    "           'VII_MIN','VIII_MIN','IX_MIN','X_MIN','XI_MIN','XII_MIN','XIII_MIN','XIV_MIN',\n",
    "   - location: 'lon','lat'\n",
    "- Qualitative\n",
    "  - building_material(9),city(11),total_floor(29),building_type(5),building_use(10),parking_way,\n",
    "    parking_area, txn_floor,'doc_rate', 'master_rate', 'bachelor_rate', 'jobschool_rate',\n",
    "       'highschool_rate', 'junior_rate', 'elementary_rate', 'born_rate',\n",
    "       'death_rate', 'marriage_rate', 'divorce_rate'\n",
    "  - village(2899)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantitative = ['txn_dt', 'building_complete_dt','parking_price','building_area','village_income_median','town_population','town_area',\n",
    "           'town_population_density','I_MIN','II_MIN','III_MIN','IV_MIN','V_MIN','VI_MIN',\n",
    "           'VII_MIN','VIII_MIN','IX_MIN','X_MIN','XI_MIN','XII_MIN','XIII_MIN','XIV_MIN',\n",
    "           'lon','lat']\n",
    "qualitative=['building_material','city','total_floor','building_type','building_use',\n",
    "             'parking_way','parking_area','txn_floor','doc_rate', 'master_rate', \n",
    "             'bachelor_rate', 'jobschool_rate','highschool_rate', 'junior_rate', \n",
    "             'elementary_rate', 'born_rate','death_rate', 'marriage_rate', 'divorce_rate',\n",
    "             'village']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Normality test\n",
    "---\n",
    "For quanntitative features, do the features follow normal distributed? The Shapior test,  `scipy.stats.shapiro`, does help to filter out the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.shapiro?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['total_price'].sample(n=5000, random_state=100).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p-value <0.01\n",
    "#test_normality = lambda x: stats.shapiro(x.fillna(0))[1] < 0.01\n",
    "#normal = pd.DataFrame(train_df['total_price'])\n",
    "stats.shapiro(np.log(train_df['total_price'].sample(n=5000, random_state=100).values))\n",
    "#print(not normal.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# p-value <0.01\n",
    "test_normality = lambda x: stats.shapiro(x.fillna(0))[1] < 0.01\n",
    "normal = pd.DataFrame(train_df[quantitative].sample(n=5000, random_state=100))\n",
    "normal = normal.apply(test_normality)\n",
    "print(not normal.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_check(y,kind='log'):\n",
    "    if kind=='log':\n",
    "       y_c=np.log(y)\n",
    "    else:\n",
    "       y_c=y \n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.subplot(121)\n",
    "    sns.distplot(y_c , fit=norm);\n",
    "    (mu, sigma) = norm.fit(y_c)\n",
    "    #print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
    "    plt.legend(['Normal dist. ($\\mu:$ {:.2f}, $\\sigma:$ {:.2f} )'.format(mu, sigma)],fontsize=14,\n",
    "            loc='best')\n",
    "    plt.title('Convert by %s' %kind)\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    #fig = plt.figure(figsize=[8,6])\n",
    "    res = stats.probplot(y_c, plot=plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=(train_df['total_price'])\n",
    "\n",
    "dist_check(y,kind='log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# too large for data skewness and kurtosis\n",
    "print(\"Skewness: %f\" % train_df['total_price'].skew()) \n",
    "print(\"Kurtosis: %f\" % train_df['total_price'].kurt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try another one in log degree, look ...\n",
    "print(\"Skewness: %f\" % np.log(train_df['total_price']).skew()) \n",
    "print(\"Kurtosis: %f\" % np.log(train_df['total_price']).kurt())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear dependings** of quantitative feature and log(taget) variable and others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "log_target=np.log(train_df['total_price'])\n",
    "\n",
    "for num, var in enumerate(quantitative[-2:]):\n",
    "    plt.subplot(1, len(quantitative[-2:]), num + 1)\n",
    "    sns.regplot(x=train_df[var], y = log_target);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "log_target=np.log(train_df['total_price'])\n",
    "n=8\n",
    "for num, var in enumerate(quantitative[3*(n-1):3*n]):\n",
    "    plt.subplot(1, len(quantitative[:3]), num + 1)\n",
    "    sns.regplot(x=train_df[var], y = log_target);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "log_target=np.log(train_df['total_price'])\n",
    "n=7\n",
    "\n",
    "for num, var in enumerate(qualitative[3*(n-1):3*n]):\n",
    "    plt.subplot(1, len(qualitative[:3]), num + 1)\n",
    "    sns.regplot(x=train_df[var], y = log_target);"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "To classify relations of the features whether could be in group, use the utilities of pandas, nuniques, value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only one class in this feature, remove it\n",
    "train_df['I_index_5000'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within the last half part of features, features should be considered in group, for instance, the features about their neighborhood infomation:\n",
    "      \n",
    "        'N_50','N_500','N_1000','N_10000' \n",
    "Group them together and name it as `N_arr`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removale=['I_index_5000','I_index_10000', ...\n",
    "         ]\n",
    "N_arr=['N_50','N_500','N_1000','N_10000']\n",
    "I_arr=['I_10','I_50','I_100','I_250','I_500','I_1000','I_5000','I_10000']\n",
    "I_ind_arr=['I_index_50','I_index_500','I_index_1000']\n",
    "II_arr=['II_10','II_50','II_100','II_250','II_500','II_1000','II_5000','II_10000']\n",
    "II_ind_arr=['II_index_50','II_index_500','II_index_1000']\n",
    "III_arr=[...]\n",
    "III_ind_arr=[...]\n",
    "\n",
    "...\n",
    "\n",
    "XIV_arr=['XIV_10','XIV_50','XIV_100','XIV_250','XIV_500','XIV_1000','XIV_5000','XIV_10000']\n",
    "XIV_ind_arr=['XIV_index_50','XIV_index_500','XIV_index_1000']\n",
    "target=['total_price']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "vars=I_ind_arr\n",
    "for num, var in enumerate(vars):\n",
    "    plt.subplot(1, len(vars), num + 1)\n",
    "    sns.regplot(x=train_df[var], y = log_target);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spearman(frame, features,target='total_price'):\n",
    "    spr = pd.DataFrame()\n",
    "    spr['feature'] = features\n",
    "    spr['spearman'] = [frame[f].corr(frame[target], 'spearman') for f in features]\n",
    "    spr = spr.sort_values('spearman')\n",
    "    plt.figure(figsize=(6, 0.25*len(features)))\n",
    "    sns.barplot(data=spr, y='feature', x='spearman', orient='h')\n",
    "    \n",
    "features = quantitative "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearman(train_df, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target=['total_price']\n",
    "plt.figure(1)\n",
    "corr = train_df[quantitative+target].corr()\n",
    "sns.heatmap(corr)\n",
    "plt.figure(2)\n",
    "corr = train_df[qual_encoded+target].corr()\n",
    "sns.heatmap(corr)\n",
    "plt.figure(3)\n",
    "corr = pd.DataFrame(np.zeros([len(quantitative)+1, len(qual_encoded)+1]), index=quantitative+target, columns=qual_encoded+['SalePrice'])\n",
    "for q1 in quantitative+target:\n",
    "    for q2 in qual_encoded+target:\n",
    "        corr.loc[q1, q2] = train_df[q1].corr(train_df[q2])\n",
    "sns.heatmap(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[set(XII_ind_arr).union(target)].corr()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
