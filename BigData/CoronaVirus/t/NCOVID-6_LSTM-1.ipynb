{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subjest\n",
    "===\n",
    "Use LSTM scheme to predict the future. Create a new directory, named \"model\", in which the trained model should be stored here.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Requirements\n",
    "---\n",
    "1. warktermark, An IPython magic extension for printing date and time stamps, version numbers, and hardware information. (install by pip)\n",
    "- tensorflow\n",
    "- keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -v -p tensorflow,sklearn,keras,scipy,pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name='../COVID-19-master/csse_covid_19_data/csse_covid_19_time_series/'\n",
    "url = dir_name+\"/time_series_covid19_confirmed_global.csv\"\n",
    "df_confirmed = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_confirmed.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "country = \"Taiwan*\"\n",
    "\n",
    "# retrieve the data\n",
    "df_confirmed1 = df_confirmed[df_confirmed[\"Country/Region\"] == country]\n",
    "\n",
    "## structuring times eries data\n",
    "df_confirmed2 = pd.DataFrame(df_confirmed1[df_confirmed1.columns[4:]].sum(),columns=[\"confirmed\"])\n",
    "df_confirmed2.index = pd.to_datetime(df_confirmed2.index)#,format='%m/%d/%y')\n",
    "df_confirmed2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_washing(df_,col=\"confirmed\",country=\"Taiwan*\"):\n",
    "    \"\"\"\n",
    "    simple washing for df\n",
    "    \"\"\"\n",
    "    df=df_.copy()\n",
    "    \n",
    "    # retrieve the data\n",
    "    df_1 = df[df[\"Country/Region\"] == country]\n",
    "\n",
    "    ## structuring times eries data\n",
    "    df_2 = pd.DataFrame(df_1[df_1.columns[4:]].sum(),columns=[col])\n",
    "    df_2.index = pd.to_datetime(df_2.index)#,format='%m/%d/%y')\n",
    "    return df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country=\"Taiwan*\"\n",
    "\n",
    "df_confirmed2 = df_washing(df_confirmed)\n",
    "df_confirmed2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for cases of dead\n",
    "\n",
    "df_dead = pd.read_csv(dir_name+\"time_series_covid19_deaths_global.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dead.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dead2 = df_washing(df_dead,col=\"dead\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### for cases of recovered\n",
    "df_recovered = pd.read_csv(dir_name+\"time_series_covid19_recovered_global.csv\")\n",
    "\n",
    "df_recovered2 = df_washing(df_recovered,col=\"recovered\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## join confirmed with dead\n",
    "df_conf_dead = df_confirmed2.join(df_dead2,how = \"inner\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## join confirmed+dead with recovered\n",
    "df_all = df_conf_dead.join(df_recovered2,how = \"inner\")\n",
    "df_all.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.plot(figsize=(14,6),kind='line', marker='o',\\\n",
    "            title=\"COVID-19 statistics at %s\" %country)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# up to 2020-04-06\n",
    "df_new = df_confirmed2[[\"confirmed\"]]\n",
    "df_new.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# daily data and i want to predict 5 days afterwards\n",
    "len(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = len(df_new)-5\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=df_new.iloc[:x]\n",
    "test = df_new.iloc[x:]\n",
    "train.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data ranges too wide to keep stationary; one of the methods is to convert them among `[0,1]`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##scale or normalize data as the data is too skewed\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(train) #find max value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_train = scaler.transform(train)#and divide every point by max value\n",
    "scaled_test = scaler.transform(test)\n",
    "print(scaled_train[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(scaled_train)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## feed in batches [t1,t2,t3,t4,t5] --> y=t6\n",
    "\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## how to decide num of inputs , \n",
    "n_input = 5  ## number of steps\n",
    "n_features = 1 ## number of features you want to predict (for univariate time series n_features=1)\n",
    "generator = TimeseriesGenerator(scaled_train,scaled_train,length = n_input,batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#62\n",
    "len(scaled_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#57\n",
    "len(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_train[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## above takes 5 inputs and predicts next point in scaled_train\n",
    "## smaller batch size leads to better trainig for time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning Moel\n",
    "---\n",
    "\n",
    "```\n",
    "Layer1 (Input Layer)   ➡︎  Layer2   ➠   ...    ➡︎ Layer (n-1)  ➡︎  Layer n (output Layer)\n",
    "      X1              P1    X2                     X(n-1)    Pn   y = Xn\n",
    "\n",
    "    Pi = Processing: AXi+bi ➧ Activate-i (relu, gelu, etc)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Activation\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    \"\"\"Gaussian Error Linear Unit.\n",
    "    This is a smoother version of the RELU.\n",
    "    Original paper: https://arxiv.org/abs/1606.08415\n",
    "    Args:\n",
    "        x: float Tensor to perform activation.\n",
    "    Returns:\n",
    "        `x` with the GELU activation applied.\n",
    "    \"\"\"\n",
    "    cdf = 0.5 * (1.0 + tf.tanh(\n",
    "        (np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3)))))\n",
    "    return x * cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= np.linspace(-3,3,101)\n",
    "Y=gelu(X)\n",
    "Y_relu=np.where(X<0,0,X)\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.title(\"Activate function relu and gelu\")\n",
    "plt.plot(X,Y,label=\"gelu\")\n",
    "plt.plot(X,Y_relu,label=\"relu\")\n",
    "plt.grid(True)\n",
    "plt.xlim([-3,3])\n",
    "plt.ylim([-2,4])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0=np.linspace(-2.5,2,101)\n",
    "y0=x0*x0*x0/2-2*x0*x0-3*x0+4.5#3*np.sin(3*x0-1)\n",
    "y_relu=np.where(y0<0,0,y0)\n",
    "y_gelu= gelu(y0)\n",
    "plt.figure(figsize=[6,8])\n",
    "plt.title(\"How Activation works in Deep earning Model\")\n",
    "plt.plot(x0,y0,label=\"$x^3/2-2x^2-3x+4.5$\",color=\"blue\")\n",
    "plt.plot(x0,y_relu,label='relu')\n",
    "plt.plot(x0,y_gelu,label='gelu',color=\"red\")\n",
    "plt.xlim([-3,3])\n",
    "plt.ylim([-6,6])\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Activation\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "\n",
    "def custom_gelu(x):    \n",
    "    return 0.5 * x * (1 + tf.tanh(tf.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))\n",
    "\n",
    "get_custom_objects().update({'custom_gelu': Activation(custom_gelu)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "LSTM:  Long Short Term Memory Networks (LSTM) models           \n",
    "\n",
    "           150 neurons      75 neurons      1 neuron\n",
    "  input ➨    LSTM       ➨   Dense Layer  ➨  output  \n",
    "```  \n",
    "1. Input and output: 5 daily data are used to predict the afterward one day data:\n",
    "$$\\text{Input: }\\mathbf{X}=[t_i,t_{i+1},t_{i+2},t_{i+3},t_{i+4}]\\Rightarrow \\text{Output: }\\mathbf{y}=[t_{i+5}]$$\n",
    "2. As above, processing in each layler is by \n",
    "  a) firstly, Linear regression;\n",
    "  b) and by activation as follows:\n",
    "  \n",
    "$$\\mathbf{X_{n\\times5}\\Rightarrow X_{n\\times5}w_{5\\times1}+b_{n\\times1}{{\\text{Activation}\\atop\\Rightarrow }}y_{n\\times1}}  $$\n",
    "\n",
    "by optimizer and loss (measurement of loss).\n",
    "3. Deep learning introduces additional neurons layer within the sequential model as follows:\n",
    "```\n",
    "         hidden layer\n",
    "             ○\n",
    "         ⤢  \n",
    " \n",
    "      ●      ...     ➠ Output\n",
    "         ⤡\n",
    "             ○\n",
    "```\n",
    "3. LSTM also keeps the long-term memory infos except the short-term memory.\n",
    "4. Activatation is an efficient nonliner mechinization, relu or gelu etc;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_model(df_):\n",
    "    \n",
    "    print(\"1. Splite Data  into train/test sets ...\\n\")\n",
    "    df=df_.copy()\n",
    "    x = len(df)-5\n",
    "    train_1=df.iloc[:x]\n",
    "    test_1 = df.iloc[x:]\n",
    "    print(\"2. Scale data into [0,1] ...\\n\")\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(train_1)\n",
    "    scaled_train_1 = scaler.transform(train_1)\n",
    "    scaled_test_1 = scaler.transform(test_1)\n",
    "    \n",
    "    print(\"3. Combine into TimeSeries Gererator for keras ...\\n\")\n",
    "    n_input = 5  ## number of steps\n",
    "    n_features = 1 ## number of features you want to predict (for univariate time series n_features=1)\n",
    "    generator = TimeseriesGenerator(scaled_train_1,scaled_train_1,length = n_input,batch_size=1)\n",
    "    \n",
    "    print(\"4. Setup LSTM Model ... \\n\")\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(150,activation=custom_gelu,input_shape=(n_input,n_features)))\n",
    "    model.add(Dropout(0.3))\n",
    "    #model.add(Dense(75, activation=\"gelu\"))\n",
    "    model.add(Dense(75, activation=custom_gelu))\n",
    "    model.add(Dense(units=1))\n",
    "    model.compile(optimizer=\"adam\",loss=\"mse\")\n",
    "    \n",
    "    print(\"5. add validate set ...\\n\")\n",
    "    \n",
    "    val_ind=len(train_1)-5\n",
    "    validation_set_1 = np.append(scaled_train_1[val_ind],scaled_test_1)\n",
    "    validation_set_1=validation_set_1.reshape(6,1)\n",
    "    validation_gen = TimeseriesGenerator(validation_set_1,validation_set_1,length=5,batch_size=1)\n",
    "    \n",
    "    print(\"6, add early stop ...\\n\")\n",
    "    early_stop = EarlyStopping(monitor='val_loss',patience=20,restore_best_weights=True)\n",
    "    print(\"7. training model ... \\n\")\n",
    "    model.fit_generator(generator,validation_data=validation_gen,epochs=100,\\\n",
    "                        callbacks=[early_stop],steps_per_epoch=10,verbose=0)\n",
    "    #pd.DataFrame(model.history.history).plot(title=\"loss vs epochs curve\",figsize=(10,6))\n",
    "    \n",
    "    \n",
    "    print(\"8. Predict the next 7 date data ...\\n\") \n",
    "    ## holding predictions\n",
    "    test_prediction = []\n",
    "\n",
    "    ##last n points from training set\n",
    "    first_eval_batch = scaled_train_1[-n_input:]\n",
    "    current_batch = first_eval_batch.reshape(1,n_input,n_features)\n",
    "    \n",
    "    for i in range(len(test_1)+7):\n",
    "        current_pred = model.predict(current_batch)[0]\n",
    "        test_prediction.append(current_pred)\n",
    "        current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1)\n",
    "    \n",
    "    true_prediction = scaler.inverse_transform(test_prediction)\n",
    "    \n",
    "    time_series_array = test_1.index\n",
    "    for k in range(0,7):\n",
    "        time_series_array = time_series_array.append(time_series_array[-1:] + pd.DateOffset(1))\n",
    "    df_forecast = pd.DataFrame(columns=[\"confirmed\",\"confirmed_predicted\"],index=time_series_array)\n",
    "    df_forecast.loc[:,\"confirmed_predicted\"] = true_prediction[:,0]\n",
    "    df_forecast.loc[:,\"confirmed\"] = test_1[\"confirmed\"]\n",
    "    \n",
    "    return df_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_confirmed_pred=LSTM_model(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_confirmed_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary\n",
    "---\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
